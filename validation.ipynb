{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers, Sequential\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model, model_from_json\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils.validation import check_is_fitted, column_or_1d\n",
    "from cophi_toolbox import preprocessing\n",
    "import pandas as pd\n",
    "from hatesonar import Sonar\n",
    "sonar = Sonar()\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import bisect\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import importlib\n",
    "import util\n",
    "importlib.reload(util)\n",
    "from util import make_w2v_embeddings, split_and_zero_padding, ManDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('sentiment_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
    "    loaded_model_sentiment = model_from_json(loaded_model_json)\n",
    "    loaded_model_sentiment.load_weights('sentiment_model.h5')\n",
    "with open('encoder.pickle', 'rb') as handle:\n",
    "    unserialized_data = pickle.load(handle)\n",
    "encoder=unserialized_data['encoder']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    " #senti should be 0 or 1, 0 is negative, 1 is positive\n",
    "\n",
    "def compareSim(input_sentence,model,datapath,senti):\n",
    "  \n",
    "    #retrive reviews based on input sentiment\n",
    "    if senti==0:\n",
    "        datafile=datapath+'/yelp_0.txt'\n",
    "    elif senti==1:\n",
    "        datafile=datapath+'/yelp_1.txt'\n",
    "\n",
    "    with open(datafile) as f:\n",
    "        datayelp=f.readlines()\n",
    "\n",
    "    #data cleaning \n",
    "    for i in range(len(datayelp)):\n",
    "        datayelp[i] = datayelp[i].replace('.','').replace('\\n','').replace('!','')\n",
    "\n",
    "    # find the top 3 similar reviews\n",
    "    result_index=[]\n",
    "    test_sentence_pairs=[]\n",
    "    for i in range(len(datayelp)):\n",
    "        test_sentence=(input_sentence,datayelp[i])\n",
    "        test_sentence_pairs.append(test_sentence)\n",
    "\n",
    "\n",
    "    embedding_dict = {}\n",
    "\n",
    "    test_df = pd.DataFrame(test_sentence_pairs, columns = ['question1','question2'])\n",
    "    for q in ['question1', 'question2']:\n",
    "        test_df[q + '_n'] = test_df[q]\n",
    "\n",
    "    test_df, embeddings = make_w2v_embeddings(embedding_dict, test_df, embedding_dim=300)\n",
    "\n",
    "    X_test = split_and_zero_padding(test_df, 10)\n",
    "\n",
    "    assert X_test['left'].shape == X_test['right'].shape\n",
    "\n",
    "    preds = list(model.predict([X_test['left'], X_test['right']]))\n",
    "\n",
    "    results = [(x, y, z) for (x, y), z in zip(test_sentence_pairs, preds)]\n",
    "    results.sort(key=itemgetter(2), reverse=True)\n",
    "\n",
    "\n",
    "    return results[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_senti(input_sentence):\n",
    "    words = list(preprocessing.tokenize(input_sentence))\n",
    "    words = list(map(lambda s: 'unknown' if s not in encoder.classes_ else s, words))\n",
    "    encoder_classes = encoder.classes_.tolist()\n",
    "    bisect.insort_left(encoder_classes, 'unknown')\n",
    "    encoder.classes_ = np.array(encoder_classes)\n",
    "    word_idx = np.array([encoder.transform(words)])\n",
    "    \n",
    "    if loaded_model_sentiment.predict_proba(word_idx)>0.6:\n",
    "            senti = 1\n",
    "    else:\n",
    "            senti = 0\n",
    "\n",
    "    return senti\n",
    "\n",
    "def pred(input_sentence,model,datapath):\n",
    "    \n",
    "    class_value = sonar.ping(text=input_sentence)['top_class']\n",
    "    if class_value == 'offensive_language':\n",
    "        senti=check_senti(input_sentence)\n",
    "        results=compareSim(input_sentence,model,datapath,senti)\n",
    "    else:\n",
    "        results='NO TOXIC'\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The food is damn good', 'this man was absolutely stellar ', array([0.9855283], dtype=float32)), ('The food is damn good', 'so yes i went there , the meal was good ', array([0.9409161], dtype=float32)), ('The food is damn good', 'the food was not super but good prices reasonable ', array([0.9269292], dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "modelpath='./en_SiameseLSTM.h5'\n",
    "datapath='.'\n",
    "model = load_model(modelpath, custom_objects={\"ManDist\": ManDist})\n",
    "input_sen='The food is damn good'\n",
    "print(pred(input_sen,model,datapath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_sentences = pd.read_csv('toxic.csv')['Toxic'].tolist()\n",
    "import random\n",
    "toxic_validation = random.sample(toxic_sentences, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath='./en_SiameseLSTM.h5'\n",
    "model = load_model(modelpath, custom_objects={\"ManDist\": ManDist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath='.'\n",
    "detoxic_list = []\n",
    "correct_list = []\n",
    "sentiment_list = []\n",
    "for sentence in toxic_validation:\n",
    "    senti = check_senti(sentence)\n",
    "    ds = pred(sentence,model,datapath)\n",
    "    detoxic_sentence = [ds[i][1] for i in range(3)]\n",
    "    detoxic_list.append(detoxic_sentence)\n",
    "    confid_score = [sonar.ping(text=ds[i][1])['classes'][1]['confidence'] for i in range(3)]\n",
    "    sentiment_score = [check_senti(ds[i][1]) for i in range(3)]\n",
    "    if  all(confid_score[i] < 0.8 for i in range(3)):\n",
    "        correct_list.append(True)\n",
    "    else:\n",
    "        correct_list.append(False) \n",
    "    if any(sentiment_score[i] == senti for i in range(3)):\n",
    "        sentiment_list.append(True)\n",
    "    else:\n",
    "        sentiment_list.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "detoxic_df = pd.DataFrame(detoxic_list,columns = ['1st','2nd','3rd'])\n",
    "origin_df = pd.DataFrame(toxic_validation, columns=['Origin'])\n",
    "correct_df = pd.DataFrame(correct_list,columns=['Detoxicity'])\n",
    "sentiment_df = pd.DataFrame(sentiment_list,columns=['Same Sentiment'])\n",
    "output_df = pd.concat([origin_df,detoxic_df,sentiment_df,correct_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Origin</th>\n",
       "      <th>1st</th>\n",
       "      <th>2nd</th>\n",
       "      <th>3rd</th>\n",
       "      <th>Same Sentiment</th>\n",
       "      <th>Detoxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last trip here, they have promos on their drin...</td>\n",
       "      <td>this location though has horribly slow service</td>\n",
       "      <td>huge fail for this type of venue</td>\n",
       "      <td>at this point they were open and would be for ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great show - laughed my ass off</td>\n",
       "      <td>since i moved here , i 've been shopping at th...</td>\n",
       "      <td>i knew i should have taken my camera</td>\n",
       "      <td>so happy i found this place</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boy, do these boys kick my ass every time I ...</td>\n",
       "      <td>the staff is always nice and they keep the sto...</td>\n",
       "      <td>it is a great hometown neighborhood bar with g...</td>\n",
       "      <td>great homemade hot breakfast sausage and a won...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>diarrhea lots, shit my pants once</td>\n",
       "      <td>extremely slow service and a rude waitress on ...</td>\n",
       "      <td>one table near us had to flag a waitress just ...</td>\n",
       "      <td>come on -- this is an `` asian bistro  ''</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fucking eat something</td>\n",
       "      <td>servers are chatting , not paying attention to...</td>\n",
       "      <td>and there 's hardly any tables to sit at</td>\n",
       "      <td>both drinks were overly sweet and barely any a...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The service sucked ass</td>\n",
       "      <td>awful awful</td>\n",
       "      <td>absolutely the worst care in all my experience...</td>\n",
       "      <td>awful acoustics</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>there is a large dance floor for some shit k...</td>\n",
       "      <td>the apricot cream cheese ones are delicious</td>\n",
       "      <td>this location is now my closest store , so i d...</td>\n",
       "      <td>so amazing with the sauce on the side</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From the valet who is running his ass off to ...</td>\n",
       "      <td>the service is always top notch and customer s...</td>\n",
       "      <td>it was truly outstanding</td>\n",
       "      <td>always a treat to have dinner at papa j 's</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The chips 'n salsa kicks ass</td>\n",
       "      <td>i really really want this place to do better</td>\n",
       "      <td>okay , i get it , they work on commission , bu...</td>\n",
       "      <td>it was really embarrassing and every time he c...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>No sweat, I could give a shit less as long a...</td>\n",
       "      <td>it had no signs of being coated and sauted to ...</td>\n",
       "      <td>this store has such bad service</td>\n",
       "      <td>this place has none of them</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"Me: What the fuck, bitch</td>\n",
       "      <td>awful awful</td>\n",
       "      <td>there is an atm , but it has a $ _num_ charge ...</td>\n",
       "      <td>i ordered a drink _num_ times and never got it</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>One of the drunken hicks actually got into a...</td>\n",
       "      <td>shortly after returning home she started to co...</td>\n",
       "      <td>when my dish comes out , it 's literally just ...</td>\n",
       "      <td>had lobster bisque soup not to good</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>And the waitstaff (Damn, I wish I could reme...</td>\n",
       "      <td>the apricot cream cheese ones are delicious</td>\n",
       "      <td>always packed here and always a wait but worth...</td>\n",
       "      <td>the mechanics are certainly the best</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"what is this guy, fucking deaf? me: \"no thank...</td>\n",
       "      <td>awful awful</td>\n",
       "      <td>this is the absolute worst steak n shake i 've...</td>\n",
       "      <td>waitress never apologized</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Manager guy, the guy who looks like a re...</td>\n",
       "      <td>there are better places to eat for the same mo...</td>\n",
       "      <td>the biggest downfall of duke 's , though , is ...</td>\n",
       "      <td>both drinks were overly sweet and barely any a...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>It was a monumental fucking embarrassment</td>\n",
       "      <td>the crab cakes were definitely frozen , and no...</td>\n",
       "      <td>when we got to the table , we were hoping for ...</td>\n",
       "      <td>the worst dental experience of my life</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>It kicked my ass, thank god my girl was drivi...</td>\n",
       "      <td>i will say make a reservation since it 's pret...</td>\n",
       "      <td>i also tried their burger and chicken sandwich...</td>\n",
       "      <td>ate here last night  fantastic food &amp; service</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Now, I like to coast, but shit</td>\n",
       "      <td>finally , i find one after _num_ minutes of wo...</td>\n",
       "      <td>absolutely terrible night</td>\n",
       "      <td>i 've eaten at various mcdonalds restaurants f...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Music was loud as shit</td>\n",
       "      <td>unfortunately the service is very hit or miss</td>\n",
       "      <td>do n't waste your time , or risk your health</td>\n",
       "      <td>the chicken was fatty and just tasted like it ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(pussy John Leguizamo) walked away pretty emb...</td>\n",
       "      <td>_num_ hour wait and then cold food served</td>\n",
       "      <td>mind you , our food still has n't came yet</td>\n",
       "      <td>if you want rock bottom food , head to rock bo...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Origin  \\\n",
       "0   Last trip here, they have promos on their drin...   \n",
       "1                     Great show - laughed my ass off   \n",
       "2     Boy, do these boys kick my ass every time I ...   \n",
       "3                   diarrhea lots, shit my pants once   \n",
       "4                               Fucking eat something   \n",
       "5                              The service sucked ass   \n",
       "6     there is a large dance floor for some shit k...   \n",
       "7    From the valet who is running his ass off to ...   \n",
       "8                        The chips 'n salsa kicks ass   \n",
       "9     No sweat, I could give a shit less as long a...   \n",
       "10                          \"Me: What the fuck, bitch   \n",
       "11    One of the drunken hicks actually got into a...   \n",
       "12    And the waitstaff (Damn, I wish I could reme...   \n",
       "13  \"what is this guy, fucking deaf? me: \"no thank...   \n",
       "14    The Manager guy, the guy who looks like a re...   \n",
       "15          It was a monumental fucking embarrassment   \n",
       "16   It kicked my ass, thank god my girl was drivi...   \n",
       "17                     Now, I like to coast, but shit   \n",
       "18                             Music was loud as shit   \n",
       "19   (pussy John Leguizamo) walked away pretty emb...   \n",
       "\n",
       "                                                  1st  \\\n",
       "0     this location though has horribly slow service    \n",
       "1   since i moved here , i 've been shopping at th...   \n",
       "2   the staff is always nice and they keep the sto...   \n",
       "3   extremely slow service and a rude waitress on ...   \n",
       "4   servers are chatting , not paying attention to...   \n",
       "5                                        awful awful    \n",
       "6        the apricot cream cheese ones are delicious    \n",
       "7   the service is always top notch and customer s...   \n",
       "8       i really really want this place to do better    \n",
       "9   it had no signs of being coated and sauted to ...   \n",
       "10                                       awful awful    \n",
       "11  shortly after returning home she started to co...   \n",
       "12       the apricot cream cheese ones are delicious    \n",
       "13                                       awful awful    \n",
       "14  there are better places to eat for the same mo...   \n",
       "15  the crab cakes were definitely frozen , and no...   \n",
       "16  i will say make a reservation since it 's pret...   \n",
       "17  finally , i find one after _num_ minutes of wo...   \n",
       "18     unfortunately the service is very hit or miss    \n",
       "19         _num_ hour wait and then cold food served    \n",
       "\n",
       "                                                  2nd  \\\n",
       "0                   huge fail for this type of venue    \n",
       "1               i knew i should have taken my camera    \n",
       "2   it is a great hometown neighborhood bar with g...   \n",
       "3   one table near us had to flag a waitress just ...   \n",
       "4           and there 's hardly any tables to sit at    \n",
       "5   absolutely the worst care in all my experience...   \n",
       "6   this location is now my closest store , so i d...   \n",
       "7                           it was truly outstanding    \n",
       "8   okay , i get it , they work on commission , bu...   \n",
       "9                    this store has such bad service    \n",
       "10  there is an atm , but it has a $ _num_ charge ...   \n",
       "11  when my dish comes out , it 's literally just ...   \n",
       "12  always packed here and always a wait but worth...   \n",
       "13  this is the absolute worst steak n shake i 've...   \n",
       "14  the biggest downfall of duke 's , though , is ...   \n",
       "15  when we got to the table , we were hoping for ...   \n",
       "16  i also tried their burger and chicken sandwich...   \n",
       "17                         absolutely terrible night    \n",
       "18      do n't waste your time , or risk your health    \n",
       "19        mind you , our food still has n't came yet    \n",
       "\n",
       "                                                  3rd  Same Sentiment  \\\n",
       "0   at this point they were open and would be for ...            True   \n",
       "1                        so happy i found this place             True   \n",
       "2   great homemade hot breakfast sausage and a won...            True   \n",
       "3           come on -- this is an `` asian bistro  ''            True   \n",
       "4   both drinks were overly sweet and barely any a...            True   \n",
       "5                                    awful acoustics             True   \n",
       "6              so amazing with the sauce on the side             True   \n",
       "7         always a treat to have dinner at papa j 's             True   \n",
       "8   it was really embarrassing and every time he c...            True   \n",
       "9                        this place has none of them             True   \n",
       "10    i ordered a drink _num_ times and never got it             True   \n",
       "11               had lobster bisque soup not to good             True   \n",
       "12              the mechanics are certainly the best             True   \n",
       "13                         waitress never apologized             True   \n",
       "14  both drinks were overly sweet and barely any a...            True   \n",
       "15            the worst dental experience of my life             True   \n",
       "16     ate here last night  fantastic food & service             True   \n",
       "17  i 've eaten at various mcdonalds restaurants f...            True   \n",
       "18  the chicken was fatty and just tasted like it ...            True   \n",
       "19  if you want rock bottom food , head to rock bo...            True   \n",
       "\n",
       "    Detoxicity  \n",
       "0         True  \n",
       "1         True  \n",
       "2         True  \n",
       "3         True  \n",
       "4         True  \n",
       "5         True  \n",
       "6         True  \n",
       "7         True  \n",
       "8         True  \n",
       "9         True  \n",
       "10        True  \n",
       "11        True  \n",
       "12        True  \n",
       "13        True  \n",
       "14        True  \n",
       "15        True  \n",
       "16        True  \n",
       "17        True  \n",
       "18        True  \n",
       "19        True  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.to_csv('output.csv',index=False)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_toxic = open('toxic_model.json', 'r')\n",
    "loaded_model_json_toxic = json_file_toxic.read()\n",
    "json_file_toxic.close()\n",
    "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
    "    loaded_model_toxic = model_from_json(loaded_model_json_toxic)\n",
    "    loaded_model_toxic.load_weights('toxic_model.h5')\n",
    "\n",
    "def compareSim_bench(input_sentence,model,datapath,senti):\n",
    "    # data and trained model should be in the same path\n",
    "    \n",
    "\n",
    "\n",
    "    #retrive reviews based on input sentiment\n",
    "    if senti==0:\n",
    "        datafile=datapath+'/yelp_0.txt'\n",
    "    elif senti==1:\n",
    "        datafile=datapath+'/yelp_1.txt'\n",
    "\n",
    "    with open(datafile) as f:\n",
    "        datayelp=f.readlines()\n",
    "\n",
    "    #data cleaning \n",
    "    for i in range(len(datayelp)):\n",
    "        datayelp[i] = datayelp[i].replace('.','').replace('\\n','').replace('!','')\n",
    "\n",
    "    # find the top 3 similar reviews\n",
    "    result_index=[]\n",
    "    test_sentence_pairs=[]\n",
    "    for i in range(len(datayelp)):\n",
    "        test_sentence=(input_sentence,datayelp[i])\n",
    "        test_sentence_pairs.append(test_sentence)\n",
    "\n",
    "    test_data_x1, test_data_x2, leaks_test = create_test_data(tokenizer,test_sentence_pairs,  siamese_config['MAX_SEQUENCE_LENGTH'])\n",
    "    preds = list(model.predict([test_data_x1, test_data_x2, leaks_test], verbose=1).ravel())\n",
    "    results = [(x, y, z) for (x, y), z in zip(test_sentence_pairs, preds)]\n",
    "    results.sort(key=itemgetter(2), reverse=True)\n",
    "\n",
    "    return results[0:3]\n",
    "\n",
    "def check_senti_bench(input_sentence):\n",
    "    words = list(preprocessing.tokenize(input_sentence))\n",
    "    words = list(map(lambda s: 'unknown' if s not in encoder.classes_ else s, words))\n",
    "    encoder_classes = encoder.classes_.tolist()\n",
    "    bisect.insort_left(encoder_classes, 'unknown')\n",
    "    encoder.classes_ = np.array(encoder_classes)\n",
    "    word_idx = np.array([encoder.transform(words)])\n",
    "    \n",
    "    if loaded_model_toxic.predict_proba(word_idx)>0.6:\n",
    "            senti = 1\n",
    "    else:\n",
    "            senti = 0\n",
    "\n",
    "    return senti\n",
    "\n",
    "\n",
    "def pred_bench(input_sentence,model,datapath):\n",
    "    \n",
    "    class_value = sonar.ping(text=input_sentence)['top_class']\n",
    "    if class_value == 'offensive_language':\n",
    "        senti=check_senti_bench(input_sentence)\n",
    "        results=compareSim_bench(input_sentence,model,datapath,senti)\n",
    "    else:\n",
    "        results='NO TOXIC'\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding_meta_data.pickle', 'rb') as handle:\n",
    "    unserialized_data = pickle.load(handle)\n",
    "tokenizer = unserialized_data['tokenizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath='./checkpoints/1549852458/lstm_50_50_0.17_0.20.h5'\n",
    "model_bench = load_model(modelpath, custom_objects={'auc':auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 214us/step\n",
      "1000/1000 [==============================] - 0s 216us/step\n",
      "1000/1000 [==============================] - 0s 221us/step\n",
      "1000/1000 [==============================] - 0s 212us/step\n",
      "1000/1000 [==============================] - 0s 214us/step\n",
      "1000/1000 [==============================] - 0s 216us/step\n",
      "1000/1000 [==============================] - 0s 206us/step\n",
      "1000/1000 [==============================] - 0s 216us/step\n",
      "1000/1000 [==============================] - 0s 220us/step\n",
      "1000/1000 [==============================] - 0s 230us/step\n",
      "1000/1000 [==============================] - 0s 210us/step\n",
      "1000/1000 [==============================] - 0s 206us/step\n",
      "1000/1000 [==============================] - 0s 211us/step\n",
      "1000/1000 [==============================] - 0s 224us/step\n",
      "1000/1000 [==============================] - 0s 231us/step\n",
      "1000/1000 [==============================] - 0s 224us/step\n",
      "1000/1000 [==============================] - 0s 219us/step\n",
      "1000/1000 [==============================] - 0s 234us/step\n",
      "1000/1000 [==============================] - 0s 236us/step\n",
      "1000/1000 [==============================] - 0s 238us/step\n"
     ]
    }
   ],
   "source": [
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):  \n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')  \n",
    "    # N = total number of negative labels  \n",
    "    N = K.sum(1 - y_true)  \n",
    "    # FP = total number of false alerts, alerts from the negative class labels  \n",
    "    FP = K.sum(y_pred - y_pred * y_true)  \n",
    "    return FP/N  \n",
    "\n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):  \n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')  \n",
    "    # P = total number of positive labels  \n",
    "    P = K.sum(y_true)  \n",
    "    # TP = total number of correct alerts, alerts from the positive class labels  \n",
    "    TP = K.sum(y_pred * y_true)  \n",
    "    return TP/P  \n",
    "\n",
    "def auc(y_true, y_pred):  \n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)  \n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)  \n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)  \n",
    "    binSizes = -(pfas[1:]-pfas[:-1])  \n",
    "    s = ptas*binSizes  \n",
    "    return K.sum(s, axis=0) \n",
    "\n",
    "from inputHandler import word_embed_meta_data, create_test_data\n",
    "import importlib\n",
    "import model\n",
    "import inputHandler\n",
    "import config\n",
    "importlib.reload(model)\n",
    "importlib.reload(config)\n",
    "importlib.reload(inputHandler)\n",
    "from inputHandler import word_embed_meta_data, create_test_data\n",
    "from config import siamese_config\n",
    "from model import SiameseBiLSTM\n",
    "\n",
    "datapath='.'\n",
    "detoxic_list = []\n",
    "correct_list = []\n",
    "score_list = []\n",
    "sentiment_list = []\n",
    "for sentence in toxic_validation:\n",
    "    ds = pred_bench(sentence,model_bench,datapath)\n",
    "    senti = check_senti(sentence)\n",
    "    try:\n",
    "        detoxic_sentence = [ds[i][1] for i in range(3)]\n",
    "        detoxic_list.append(detoxic_sentence)\n",
    "        #check if the results have been detoxicated\n",
    "        class_value = [sonar.ping(text=ds[i][1])['top_class'] for i in range(3)]\n",
    "        sentiment_score = [check_senti(ds[i][1]) for i in range(3)]\n",
    "        if  all(class_value[i] != 'offensive_language' for i in range(3)):\n",
    "            correct_list.append(True)\n",
    "        else:\n",
    "            correct_list.append(False) \n",
    "        if any(sentiment_score[i] == senti for i in range(3)):\n",
    "            sentiment_list.append(True)\n",
    "        else:\n",
    "            sentiment_list.append(False)\n",
    "    except:\n",
    "        detoxic_list.append(ds)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "detoxic_df = pd.DataFrame(detoxic_list,columns = ['1st','2nd','3rd'])\n",
    "origin_df_bench = pd.DataFrame(toxic_validation, columns=['Origin'])\n",
    "correct_df = pd.DataFrame(correct_list,columns=['Detoxicity'])\n",
    "sentiment_df = pd.DataFrame(sentiment_list,columns=['Same Sentiment'])\n",
    "output_df_bench = pd.concat([origin_df_bench,detoxic_df,sentiment_df, correct_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Origin</th>\n",
       "      <th>1st</th>\n",
       "      <th>2nd</th>\n",
       "      <th>3rd</th>\n",
       "      <th>Same Sentiment</th>\n",
       "      <th>Detoxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last trip here, they have promos on their drin...</td>\n",
       "      <td>so on to the hoagies , the italian is general ...</td>\n",
       "      <td>nothing really special &amp; not worthy of the $ _...</td>\n",
       "      <td>second , the steak hoagie , it is atrocious</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great show - laughed my ass off</td>\n",
       "      <td>so on to the hoagies , the italian is general ...</td>\n",
       "      <td>nothing really special &amp; not worthy of the $ _...</td>\n",
       "      <td>second , the steak hoagie , it is atrocious</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boy, do these boys kick my ass every time I ...</td>\n",
       "      <td>i was sadly mistaken</td>\n",
       "      <td>i had to pay $ _num_ to add cheese to the hoagie</td>\n",
       "      <td>i was not going to pay for the dressing on the...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>diarrhea lots, shit my pants once</td>\n",
       "      <td>so on to the hoagies , the italian is general ...</td>\n",
       "      <td>nothing really special &amp; not worthy of the $ _...</td>\n",
       "      <td>second , the steak hoagie , it is atrocious</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fucking eat something</td>\n",
       "      <td>so on to the hoagies , the italian is general ...</td>\n",
       "      <td>nothing really special &amp; not worthy of the $ _...</td>\n",
       "      <td>second , the steak hoagie , it is atrocious</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The service sucked ass</td>\n",
       "      <td>so on to the hoagies , the italian is general ...</td>\n",
       "      <td>nothing really special &amp; not worthy of the $ _...</td>\n",
       "      <td>second , the steak hoagie , it is atrocious</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>there is a large dance floor for some shit k...</td>\n",
       "      <td>minimal meat and a ton of shredded lettuce</td>\n",
       "      <td>she told me there was a charge for the dressin...</td>\n",
       "      <td>not a call , not the estimate , nothing</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From the valet who is running his ass off to ...</td>\n",
       "      <td>so on to the hoagies , the italian is general ...</td>\n",
       "      <td>nothing really special &amp; not worthy of the $ _...</td>\n",
       "      <td>second , the steak hoagie , it is atrocious</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The chips 'n salsa kicks ass</td>\n",
       "      <td>so on to the hoagies , the italian is general ...</td>\n",
       "      <td>nothing really special &amp; not worthy of the $ _...</td>\n",
       "      <td>second , the steak hoagie , it is atrocious</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>No sweat, I could give a shit less as long a...</td>\n",
       "      <td>i never imagined how confusing a question that...</td>\n",
       "      <td>my husband &amp; i ate here a couple weeks ago for...</td>\n",
       "      <td>i ordered a drink _num_ times and never got it</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"Me: What the fuck, bitch</td>\n",
       "      <td>so on to the hoagies , the italian is general ...</td>\n",
       "      <td>nothing really special &amp; not worthy of the $ _...</td>\n",
       "      <td>second , the steak hoagie , it is atrocious</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>One of the drunken hicks actually got into a...</td>\n",
       "      <td>minimal meat and a ton of shredded lettuce</td>\n",
       "      <td>she told me there was a charge for the dressin...</td>\n",
       "      <td>not a call , not the estimate , nothing</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>And the waitstaff (Damn, I wish I could reme...</td>\n",
       "      <td>i was sadly mistaken</td>\n",
       "      <td>i had to pay $ _num_ to add cheese to the hoagie</td>\n",
       "      <td>i was not going to pay for the dressing on the...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"what is this guy, fucking deaf? me: \"no thank...</td>\n",
       "      <td>i was sadly mistaken</td>\n",
       "      <td>i had to pay $ _num_ to add cheese to the hoagie</td>\n",
       "      <td>i was not going to pay for the dressing on the...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Manager guy, the guy who looks like a re...</td>\n",
       "      <td>minimal meat and a ton of shredded lettuce</td>\n",
       "      <td>she told me there was a charge for the dressin...</td>\n",
       "      <td>not a call , not the estimate , nothing</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>It was a monumental fucking embarrassment</td>\n",
       "      <td>minimal meat and a ton of shredded lettuce</td>\n",
       "      <td>she told me there was a charge for the dressin...</td>\n",
       "      <td>not a call , not the estimate , nothing</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>It kicked my ass, thank god my girl was drivi...</td>\n",
       "      <td>so on to the hoagies , the italian is general ...</td>\n",
       "      <td>nothing really special &amp; not worthy of the $ _...</td>\n",
       "      <td>second , the steak hoagie , it is atrocious</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Now, I like to coast, but shit</td>\n",
       "      <td>i was sadly mistaken</td>\n",
       "      <td>i had to pay $ _num_ to add cheese to the hoagie</td>\n",
       "      <td>i was not going to pay for the dressing on the...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Music was loud as shit</td>\n",
       "      <td>so on to the hoagies , the italian is general ...</td>\n",
       "      <td>nothing really special &amp; not worthy of the $ _...</td>\n",
       "      <td>second , the steak hoagie , it is atrocious</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(pussy John Leguizamo) walked away pretty emb...</td>\n",
       "      <td>so on to the hoagies , the italian is general ...</td>\n",
       "      <td>nothing really special &amp; not worthy of the $ _...</td>\n",
       "      <td>second , the steak hoagie , it is atrocious</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Origin  \\\n",
       "0   Last trip here, they have promos on their drin...   \n",
       "1                     Great show - laughed my ass off   \n",
       "2     Boy, do these boys kick my ass every time I ...   \n",
       "3                   diarrhea lots, shit my pants once   \n",
       "4                               Fucking eat something   \n",
       "5                              The service sucked ass   \n",
       "6     there is a large dance floor for some shit k...   \n",
       "7    From the valet who is running his ass off to ...   \n",
       "8                        The chips 'n salsa kicks ass   \n",
       "9     No sweat, I could give a shit less as long a...   \n",
       "10                          \"Me: What the fuck, bitch   \n",
       "11    One of the drunken hicks actually got into a...   \n",
       "12    And the waitstaff (Damn, I wish I could reme...   \n",
       "13  \"what is this guy, fucking deaf? me: \"no thank...   \n",
       "14    The Manager guy, the guy who looks like a re...   \n",
       "15          It was a monumental fucking embarrassment   \n",
       "16   It kicked my ass, thank god my girl was drivi...   \n",
       "17                     Now, I like to coast, but shit   \n",
       "18                             Music was loud as shit   \n",
       "19   (pussy John Leguizamo) walked away pretty emb...   \n",
       "\n",
       "                                                  1st  \\\n",
       "0   so on to the hoagies , the italian is general ...   \n",
       "1   so on to the hoagies , the italian is general ...   \n",
       "2                               i was sadly mistaken    \n",
       "3   so on to the hoagies , the italian is general ...   \n",
       "4   so on to the hoagies , the italian is general ...   \n",
       "5   so on to the hoagies , the italian is general ...   \n",
       "6         minimal meat and a ton of shredded lettuce    \n",
       "7   so on to the hoagies , the italian is general ...   \n",
       "8   so on to the hoagies , the italian is general ...   \n",
       "9   i never imagined how confusing a question that...   \n",
       "10  so on to the hoagies , the italian is general ...   \n",
       "11        minimal meat and a ton of shredded lettuce    \n",
       "12                              i was sadly mistaken    \n",
       "13                              i was sadly mistaken    \n",
       "14        minimal meat and a ton of shredded lettuce    \n",
       "15        minimal meat and a ton of shredded lettuce    \n",
       "16  so on to the hoagies , the italian is general ...   \n",
       "17                              i was sadly mistaken    \n",
       "18  so on to the hoagies , the italian is general ...   \n",
       "19  so on to the hoagies , the italian is general ...   \n",
       "\n",
       "                                                  2nd  \\\n",
       "0   nothing really special & not worthy of the $ _...   \n",
       "1   nothing really special & not worthy of the $ _...   \n",
       "2   i had to pay $ _num_ to add cheese to the hoagie    \n",
       "3   nothing really special & not worthy of the $ _...   \n",
       "4   nothing really special & not worthy of the $ _...   \n",
       "5   nothing really special & not worthy of the $ _...   \n",
       "6   she told me there was a charge for the dressin...   \n",
       "7   nothing really special & not worthy of the $ _...   \n",
       "8   nothing really special & not worthy of the $ _...   \n",
       "9   my husband & i ate here a couple weeks ago for...   \n",
       "10  nothing really special & not worthy of the $ _...   \n",
       "11  she told me there was a charge for the dressin...   \n",
       "12  i had to pay $ _num_ to add cheese to the hoagie    \n",
       "13  i had to pay $ _num_ to add cheese to the hoagie    \n",
       "14  she told me there was a charge for the dressin...   \n",
       "15  she told me there was a charge for the dressin...   \n",
       "16  nothing really special & not worthy of the $ _...   \n",
       "17  i had to pay $ _num_ to add cheese to the hoagie    \n",
       "18  nothing really special & not worthy of the $ _...   \n",
       "19  nothing really special & not worthy of the $ _...   \n",
       "\n",
       "                                                  3rd  Same Sentiment  \\\n",
       "0        second , the steak hoagie , it is atrocious            False   \n",
       "1        second , the steak hoagie , it is atrocious             True   \n",
       "2   i was not going to pay for the dressing on the...           False   \n",
       "3        second , the steak hoagie , it is atrocious             True   \n",
       "4        second , the steak hoagie , it is atrocious            False   \n",
       "5        second , the steak hoagie , it is atrocious             True   \n",
       "6            not a call , not the estimate , nothing            False   \n",
       "7        second , the steak hoagie , it is atrocious            False   \n",
       "8        second , the steak hoagie , it is atrocious            False   \n",
       "9     i ordered a drink _num_ times and never got it             True   \n",
       "10       second , the steak hoagie , it is atrocious            False   \n",
       "11           not a call , not the estimate , nothing             True   \n",
       "12  i was not going to pay for the dressing on the...           False   \n",
       "13  i was not going to pay for the dressing on the...            True   \n",
       "14           not a call , not the estimate , nothing             True   \n",
       "15           not a call , not the estimate , nothing             True   \n",
       "16       second , the steak hoagie , it is atrocious             True   \n",
       "17  i was not going to pay for the dressing on the...            True   \n",
       "18       second , the steak hoagie , it is atrocious            False   \n",
       "19       second , the steak hoagie , it is atrocious            False   \n",
       "\n",
       "    Detoxicity  \n",
       "0         True  \n",
       "1         True  \n",
       "2         True  \n",
       "3         True  \n",
       "4         True  \n",
       "5         True  \n",
       "6         True  \n",
       "7         True  \n",
       "8         True  \n",
       "9         True  \n",
       "10        True  \n",
       "11        True  \n",
       "12        True  \n",
       "13        True  \n",
       "14        True  \n",
       "15        True  \n",
       "16        True  \n",
       "17        True  \n",
       "18        True  \n",
       "19        True  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df_bench.to_csv('output_bench.csv',index=False)\n",
    "output_df_bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The food is damn shit', 'awful awful ', array([0.96039075], dtype=float32)), ('The food is damn shit', 'no offer to refill drinks or for water ', array([0.94932276], dtype=float32)), ('The food is damn shit', 'both drinks were overly sweet and barely any alcohol ', array([0.9305702], dtype=float32))]\n",
      "1000/1000 [==============================] - 0s 228us/step\n",
      "[('The food is damn shit', 'so on to the hoagies , the italian is general run of the mill ', 0.41599172), ('The food is damn shit', 'nothing really special & not worthy of the $ _num_ price tag ', 0.41599172), ('The food is damn shit', 'second , the steak hoagie , it is atrocious ', 0.41599172)]\n"
     ]
    }
   ],
   "source": [
    "input_sen='The food is damn shit'\n",
    "print(pred(input_sen,model,datapath))\n",
    "print(pred_bench(input_sen,model_bench,datapath))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
